{"cells":[{"cell_type":"markdown","metadata":{"id":"csP4gW2F21Iv"},"source":["# **LSTM Model**"]},{"cell_type":"markdown","metadata":{"id":"ctRCL-c07dnH"},"source":["# **Data Format:**\n","# **Columns: Text, Label**\n","\n","|  TEXT  |   LABEL   | \n","|--------|-----------|\n","| TEXT_1 |  LABEL_1  |\n","| TEXT_2 |  LABEL_1  |\n","| TEXT_3 |  LABEL_2  |"]},{"cell_type":"markdown","metadata":{"id":"rKQAQkbntUC0"},"source":["# **Load Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKmLiH-33wSp"},"outputs":[],"source":["from google.colab import files\n","files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCWvfrZVouzO"},"outputs":[],"source":["import pandas as pd\n","csv_path = \"\"\n","data = pd.read_csv(csv_path)"]},{"cell_type":"markdown","metadata":{"id":"xiZCCA2_4i7g"},"source":["# **Pre-Processing Columns**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAw5bMGJ4Bze"},"outputs":[],"source":["data = data.drop(columns=['segment', 'speaker'])\n","data.columns = ['text', 'label']\n","data.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q2D96EGi5VXY"},"outputs":[],"source":["data.label.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"RglXZ8cC21JF"},"source":["# **Reduce Data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RiXSyxc15pj0"},"outputs":[],"source":["df = data.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PfMVQ9U21JF","tags":[]},"outputs":[],"source":["import random\n","\n","def reduceData(df, n=10000000):\n","    dic = df.groupby(by=\"label\").groups\n","    selected_texts = []\n","    selected_labels = []\n","    selected_id = []\n","    for k in dic.keys():\n","        if (len(dic[k]) > n):\n","            dic[k] = random.sample(list(dic[k]), n)\n","        for i in dic[k]:\n","            selected_labels.append(k)\n","            selected_texts.append(df.text[i])\n","    return pd.DataFrame(data={\"text\": selected_texts, \"label\": selected_labels})\n","\n","df = reduceData(df, 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"671-162W5uqe"},"outputs":[],"source":["df.label.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"vVyhSJ9421JH"},"source":["# **Text Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgOh_Ghx21JH"},"outputs":[],"source":["import functools\n","from keras.preprocessing.sequence import pad_sequences\n","\n","def vectorizer(texts):\n","    dic = {}\n","    r = []\n","    count = 1\n","    for t in texts:\n","        text = []\n","        for w in t.split(\" \"):\n","            if w in dic:\n","                text.append(dic[w])\n","            else:\n","                dic[w] = count\n","                text.append(dic[w])\n","                count += 1\n","        r.append(text)\n","    return r, dic\n","\n","def textPreprocessing(texts):\n","    texts, dic = vectorizer(texts)\n","    vocab_size = len(dic.keys())\n","    max = len(functools.reduce(lambda a, b: a if len(a) > len(b) else b, texts))\n","    texts = pad_sequences(texts, maxlen = max, padding= \"pre\")\n","    return texts, vocab_size, max\n","\n","texts, vocab_size, max = textPreprocessing(df.text.values)"]},{"cell_type":"markdown","metadata":{"id":"Dx-e-Om021JH"},"source":["# **Labels Preprocessing**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6om2wUc21JI"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import OneHotEncoder as OHE\n","\n","def labelsPreprocessing(labels):\n","    encoder = OHE().fit(np.array(labels).reshape(-1,1))\n","    labels = encoder.transform(np.array(labels).reshape(-1,1)).toarray()\n","    return labels, encoder\n","\n","labels, encoder = labelsPreprocessing(df.label.values)"]},{"cell_type":"markdown","metadata":{"id":"l6GssP5d21JJ","tags":[]},"source":["# **Training Model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvwCY8D121JJ"},"outputs":[],"source":["from tensorflow.keras.layers import Dense, LSTM, BatchNormalization, Dropout, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import CategoricalCrossentropy as CC\n","from tensorflow.keras.activations import relu, softmax\n","from tensorflow.keras.initializers import he_uniform, glorot_uniform\n","from tensorflow.keras.metrics import AUC\n","from tensorflow.keras import Model\n","from tensorflow.keras.regularizers import l2\n","\n","\n","class LSTMModel(object):\n","    \n","    def build_model(self, input_dim, output_shape, steps, dropout_rate, kernel_regularizer, bias_regularizer):\n","        input_layer= Input(shape=(steps, input_dim))\n","        \n","        #make lstm_layer\n","        lstm = LSTM(units= steps)(input_layer)\n","\n","        dense_1 = Dense(output_shape, kernel_initializer = he_uniform(),\n","                       bias_initializer= \"zeros\", \n","                       kernel_regularizer= l2(l = kernel_regularizer),\n","                       bias_regularizer= l2(l = bias_regularizer))(lstm)\n","        x = BatchNormalization()(dense_1)\n","\n","        x = relu(x)\n","        x = Dropout(rate = dropout_rate)(x)\n","\n","        o = Dense(output_shape, kernel_initializer= glorot_uniform(),\n","                 bias_initializer= \"zeros\", \n","                 kernel_regularizer= l2(l = kernel_regularizer), \n","                 bias_regularizer= l2(l = bias_regularizer))(dense_1)\n","        o = BatchNormalization()(o)\n","\n","        output = softmax(o, axis= 1)\n","        loss = CC()\n","        metrics = AUC()\n","        optimizer = Adam()\n","\n","        self.model= Model(inputs= [input_layer], outputs= [output])\n","        self.model.compile(optimizer= optimizer, loss= loss, metrics= [metrics])\n","        \n","        \n","    def train(self, x, y, validation_split, epochs):\n","        self.model.fit(x, y, validation_split = validation_split, epochs= epochs)\n","        \n","    def predict(self, x):\n","        return self.model.predict(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SE3JgK0D21JK"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","\n","def transform_x(data, len_keys):\n","    output_shape = [data.shape[0],\n","                  data.shape[1],\n","                  len_keys]\n","    results= np.zeros(output_shape)\n","    \n","    for i in range(data.shape[0]):\n","        for ii in range(data.shape[1]):\n","            results[i, ii, data[i,ii]-1]= 1\n","    return results\n","\n","def generateModel(x_transformed, output_shape):\n","    steps = x_transformed.shape[1]\n","    dim = x_transformed.shape[2]\n","\n","    model = LSTMModel()\n","    model.build_model(input_dim= dim,\n","                      output_shape = output_shape,\n","                      steps = steps, \n","                      dropout_rate = 0.7, \n","                      bias_regularizer = 0.3, \n","                      kernel_regularizer = 0.3)\n","    return model\n","\n","def plotConfusionMatrix(y_true, y_pred):\n","    data = {\n","      'real_value': y_true,\n","      'predicted': y_pred\n","    }\n","    fig, ax = plt.subplots(figsize=(14,14))\n","    df = pd.DataFrame(data, columns=['real_value','predicted'])\n","    confusion_matrix = pd.crosstab(df['real_value'], df['predicted'], rownames=['Real'], colnames=['Predicted'])\n","    sn.heatmap(confusion_matrix, annot=True, cbar=False, fmt='g')\n","    plt.show()\n","\n","def runModel(texts, labels, vocab_size, output_length, encoder):\n","    X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=3)\n","    xtr_transformed = transform_x(X_train, vocab_size)\n","    xts_transformed = transform_x(X_test, vocab_size)\n","    \n","    model = generateModel(xtr_transformed, output_length)\n","    model.train(xtr_transformed, y_train, 0.2, 30)\n","    \n","    prediction_test = encoder.inverse_transform(model.predict(xts_transformed))\n","    print(classification_report(encoder.inverse_transform(y_test), prediction_test))\n","    \n","    y_true = list (map(lambda l: l[0], encoder.inverse_transform(y_test)))\n","    y_pred = list (map(lambda l: l[0], prediction_test))\n","    plotConfusionMatrix(y_true, y_pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i7DFxccF21JK","tags":[]},"outputs":[],"source":["output_length = len(df.label.unique())\n","runModel(texts, labels, vocab_size, output_length, encoder)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"lstm_model.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
