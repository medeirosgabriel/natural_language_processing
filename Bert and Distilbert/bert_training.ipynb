{"cells":[{"cell_type":"markdown","id":"7d7af096-e29b-49d0-ad5d-8682159ea8c4","metadata":{"id":"7d7af096-e29b-49d0-ad5d-8682159ea8c4"},"source":["# **BERT Multilabel Training**"]},{"cell_type":"markdown","id":"58160f17-2a9b-45e1-a06e-d3fee543fef1","metadata":{"id":"58160f17-2a9b-45e1-a06e-d3fee543fef1"},"source":["# **Data Format:**\n","# **Columns: Text, Labels**\n","## **Text: tuple of strings - Representing the context**\n","## **Tuple length 2 -> Context 2**\n","## **Tuple length 3 -> Context 3**\n","## **...**\n","## **Label: tuple of labels - Representing the multilabel classification**\n","\n","|       TEXT       |   LABEL   | \n","|------------------|-----------|\n","| (TEXT_1, TEXT_2) |  (LABEL_1, LABEL_2, ...)  |\n","| (TEXT_1, TEXT_2) |  (LABEL_1, LABEL_3, ...)  |\n","| (TEXT_1, TEXT_2) |  (LABEL_3)                |"]},{"cell_type":"code","source":["from google.colab import files\n","files.upload()"],"metadata":{"id":"eQHgC3T8j50s"},"id":"eQHgC3T8j50s","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","csv_path = \"pizza-ordering-tm-1-2019-concat.csv\"\n","df = pd.read_csv(csv_path)\n","df.head(2)"],"metadata":{"id":"H441twN79sYe"},"id":"H441twN79sYe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = df.drop(['segment', 'speaker'], axis=1)\n","df.intent = df.intent.apply(lambda x: \".\".join(x.split(\".\")[0:2]))\n","df.head(2)"],"metadata":{"id":"BALY4BgsBHs6"},"id":"BALY4BgsBHs6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns = ['text', 'label']\n","df.label = df.label.apply(lambda x: (x,))\n","df.head()"],"metadata":{"id":"CQcZdiHJj6Ch"},"id":"CQcZdiHJj6Ch","execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.label.value_counts()"],"metadata":{"id":"wZfujh9hkCE2"},"id":"wZfujh9hkCE2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Reduce Data**"],"metadata":{"id":"R9YSiC9qDcsW"},"id":"R9YSiC9qDcsW"},{"cell_type":"code","source":["import random\n","\n","def reduceData(df, n=10000000):\n","    dic = df.groupby(by=\"label\").groups\n","    selected_texts = []\n","    selected_labels = []\n","    selected_id = []\n","    for k in dic.keys():\n","        if (len(dic[k]) > n):\n","            dic[k] = random.sample(list(dic[k]), n)\n","        for i in dic[k]:\n","            selected_labels.append(k)\n","            selected_texts.append(df.text[i])\n","    return pd.DataFrame(data={\"text\": selected_texts, \"label\": selected_labels})\n","\n","df = reduceData(df, 2000)\n","df.label.value_counts()"],"metadata":{"id":"nfKq2bCX-U4n"},"id":"nfKq2bCX-U4n","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"77e42867-b3ff-4545-9a78-3c3c577bee51","metadata":{"id":"77e42867-b3ff-4545-9a78-3c3c577bee51"},"source":["# **Processing Labels**\n","# **Example: (label_1, label_3) -> (1, 0, 1)**\n","# **Transform labels to use in the Pytorch Model**"]},{"cell_type":"code","execution_count":null,"id":"6096a46e-f55f-4160-931c-a229c9b4834d","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[],"id":"6096a46e-f55f-4160-931c-a229c9b4834d"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def labelsPreprocessing(labels):\n","    mlb = MultiLabelBinarizer()\n","    labels = mlb.fit_transform(labels)\n","    return labels, mlb\n","\n","labels, encoder = labelsPreprocessing(list(df.label.values))\n","df.label = list(map(lambda x: tuple(x), labels))\n","df.label.value_counts()"]},{"cell_type":"markdown","id":"071ccff0-09db-478c-bae5-2a5651961c26","metadata":{"id":"071ccff0-09db-478c-bae5-2a5651961c26"},"source":["# **Model Configuration - Pytorch**"]},{"cell_type":"code","source":["!pip install torch\n","!pip install transformers"],"metadata":{"id":"85pwgUcmkHkH"},"id":"85pwgUcmkHkH","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"59d1e77c-9894-4a97-bfbe-d07dbeb01a01","metadata":{"tags":[],"id":"59d1e77c-9894-4a97-bfbe-d07dbeb01a01"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, BertConfig"]},{"cell_type":"markdown","id":"cd09f2c6-d00f-4b62-ae37-2a37a215c453","metadata":{"id":"cd09f2c6-d00f-4b62-ae37-2a37a215c453"},"source":["# **Select the hardware to training the pytorch model**"]},{"cell_type":"code","execution_count":null,"id":"fd1a5053-1f9b-44c5-9172-be8bef68c60e","metadata":{"tags":[],"id":"fd1a5053-1f9b-44c5-9172-be8bef68c60e"},"outputs":[],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu' # CPU OR GPU"]},{"cell_type":"code","execution_count":null,"id":"b0378629-17f6-4c21-ae05-bf5d9ac09314","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"b0378629-17f6-4c21-ae05-bf5d9ac09314"},"outputs":[],"source":["MAX_LEN = 512 # Tokens Length\n","TRAIN_BATCH_SIZE = 8 # Train Batch Size\n","VALID_BATCH_SIZE = 4 # Valid Batch Size\n","EPOCHS = 10 # Epochs\n","LEARNING_RATE = 1e-05 # Learning Rate\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Distilbert Tokenizer"]},{"cell_type":"markdown","id":"5afe70ae-4492-4d05-84e4-c5abc7b2d9e9","metadata":{"id":"5afe70ae-4492-4d05-84e4-c5abc7b2d9e9"},"source":["# **Data Configuration to Pytorch Model Input**"]},{"cell_type":"code","execution_count":null,"id":"287d2ebe-7611-4e9a-9ef0-4156b99066cf","metadata":{"tags":[],"id":"287d2ebe-7611-4e9a-9ef0-4156b99066cf"},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        # Class Attributes\n","        self.tokenizer = tokenizer # Tokenizer\n","        self.data = dataframe # All Data\n","        self.text = dataframe.text # Select text column\n","        self.label = self.data.label # Labels\n","        self.max_len = max_len # Max length of token list\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        \n","        # Build a context with sentences\n","        \n","        t = self.text[index] # Tuple (sentence_1, sentence_2, sentence_3)\n","        token_text = \"[SEP]\".join(t) + \"[SEP]\" # Join sentences\n","        final_text = \"[CLS]\" + token_text # Put [CLS] token at the beginning of the sentence \n","            \n","        inputs = tokenizer.encode_plus(\n","            final_text, # Text with special tokens ([CLS] and )\n","            None,\n","            add_special_tokens=False, # [CLS] and [SEP]\n","            max_length=self.max_len, # Select Max Length\n","            pad_to_max_length=True, # Complete Padding with ZEROS\n","            return_token_type_ids=None, # Token Type Ids -> Identify sentences separated by [SEP] token\n","            truncation=True\n","        )\n","        \n","        # Ids = TOKEN IDS\n","        ids = inputs['input_ids']\n","        # MASK = MASK of words if you are using\n","        mask = inputs['attention_mask']\n","        # Token Type Ids = Diff Sentences\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        \n","        # Format the output for the model training\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'label': torch.tensor(self.label[index], dtype=torch.float),\n","            'sentence': final_text\n","        }"]},{"cell_type":"markdown","id":"c094fccd-178d-40b2-8a7e-580ba54f2359","metadata":{"id":"c094fccd-178d-40b2-8a7e-580ba54f2359"},"source":["# **Data Configuration To Pytorch**"]},{"cell_type":"code","execution_count":null,"id":"56f9d34c-e5b5-43d2-8bb7-0b678260676b","metadata":{"id":"56f9d34c-e5b5-43d2-8bb7-0b678260676b"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split dataset according to LABEL column\n","train_dataset, test_dataset = train_test_split(df, test_size = 0.2, stratify=df['label'])\n","train_dataset = train_dataset.reset_index().drop(columns=[\"index\"], axis=1)\n","test_dataset = test_dataset.reset_index().drop(columns=[\"index\"], axis=1)"]},{"cell_type":"markdown","id":"10f5e16e-2e2f-430b-88f2-4b0f60545393","metadata":{"id":"10f5e16e-2e2f-430b-88f2-4b0f60545393"},"source":["# **Dataset Shapes - full, train and test**"]},{"cell_type":"code","execution_count":null,"id":"5ad02650-9160-4b75-b475-1eb51b31381c","metadata":{"id":"5ad02650-9160-4b75-b475-1eb51b31381c"},"outputs":[],"source":["print(\"FULL Dataset: {}\".format(df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(test_dataset.shape))\n","\n","training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n","testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"]},{"cell_type":"markdown","id":"b77cd766-bfee-4e45-8fb6-fe83d00503a2","metadata":{"id":"b77cd766-bfee-4e45-8fb6-fe83d00503a2"},"source":["# **Train and Test Parameters**"]},{"cell_type":"code","execution_count":null,"id":"e6d57fc7-38db-48a1-a4ba-3fbbf0155624","metadata":{"tags":[],"id":"e6d57fc7-38db-48a1-a4ba-3fbbf0155624"},"outputs":[],"source":["train_params = {\n","    'batch_size': TRAIN_BATCH_SIZE,           \n","    'shuffle': True,           \n","    'num_workers': 0\n","}\n","\n","test_params = {\n","    'batch_size': VALID_BATCH_SIZE,\n","    'shuffle': True,\n","    'num_workers': 0\n","}\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"]},{"cell_type":"markdown","id":"657e88b0-5364-4287-b52a-61ef62b85162","metadata":{"id":"657e88b0-5364-4287-b52a-61ef62b85162"},"source":["# **Build DistilBERT Model**"]},{"cell_type":"code","execution_count":null,"id":"93895420-b598-419d-96e2-d0526c3aa559","metadata":{"jupyter":{"outputs_hidden":true},"tags":[],"id":"93895420-b598-419d-96e2-d0526c3aa559"},"outputs":[],"source":["class BERTClass(torch.nn.Module):\n","    def __init__(self, num_labels):\n","        super(BERTClass, self).__init__()\n","        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n","        self.l2 = torch.nn.Dropout(0.3)\n","        self.l3 = torch.nn.Linear(768, num_labels)\n","    \n","    def forward(self, ids, mask, token_type_ids):\n","        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n","        output_2 = self.l2(output_1)\n","        output = self.l3(output_2)\n","        return output\n","\n","NUM_LABELS = len(train_dataset.label.values[0])\n","model = BERTClass(NUM_LABELS)\n","model.to(device) # Choose the device (CPU or GPU)"]},{"cell_type":"markdown","id":"7a8acd32-f037-4301-aa85-97e87048646c","metadata":{"id":"7a8acd32-f037-4301-aa85-97e87048646c"},"source":["# **DistilBERT Model Loss Function**"]},{"cell_type":"code","execution_count":null,"id":"a4592956-e8cf-47a3-bc56-25ed2825ba5a","metadata":{"id":"a4592956-e8cf-47a3-bc56-25ed2825ba5a"},"outputs":[],"source":["def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n","\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"markdown","id":"6770c221-5f5e-4e02-9d20-834126845272","metadata":{"id":"6770c221-5f5e-4e02-9d20-834126845272"},"source":["# **Training DistilBERT Model**"]},{"cell_type":"code","execution_count":null,"id":"0743202c-dfc4-4e4e-875a-59b729b4dde6","metadata":{"id":"0743202c-dfc4-4e4e-875a-59b729b4dde6"},"outputs":[],"source":["def train(epoch):\n","    model.train()\n","    for _,data in enumerate(training_loader, 0):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        labels = data['label'].to(device, dtype = torch.float)\n","\n","        outputs = model(ids, mask, token_type_ids)\n","\n","        optimizer.zero_grad()\n","        loss = loss_fn(outputs, labels)\n","        \n","        if _%5000==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()"]},{"cell_type":"code","execution_count":null,"id":"89d0f553-42eb-4838-a739-474766b9024f","metadata":{"tags":[],"id":"89d0f553-42eb-4838-a739-474766b9024f"},"outputs":[],"source":["for epoch in range(EPOCHS):\n","  train(epoch)"]},{"cell_type":"markdown","id":"26f5d4fc-ff5e-4012-a01e-ee3c0aaab6d2","metadata":{"id":"26f5d4fc-ff5e-4012-a01e-ee3c0aaab6d2"},"source":["# **Predict Test Data - Model**"]},{"cell_type":"code","execution_count":null,"id":"a2835943-d20c-4681-9656-c154f7ee8b8d","metadata":{"id":"a2835943-d20c-4681-9656-c154f7ee8b8d"},"outputs":[],"source":["def validation():\n","    model.eval()\n","    fin_targets = []\n","    fin_outputs = []\n","    sentences = []\n","    annotation_ids = []\n","    with torch.no_grad():\n","        for _, data in enumerate(testing_loader, 0):\n","            ids = data['ids'].to(device, dtype = torch.long) # Tokens\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long) # Masks\n","            targets = data['label'].to(device, dtype = torch.float) # True Label\n","            outputs = model(ids, mask, token_type_ids) # Predict\n","            # Prepare Output in lists\n","            sentences.extend(data['sentence']) # predicted texts\n","            fin_targets.extend(targets.cpu().detach().numpy().tolist())  # true labels of predicted texts\n","            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())  # predicted labels of predicted texts\n","    return fin_outputs, fin_targets, sentences # Output, True Label, Sentences"]},{"cell_type":"code","execution_count":null,"id":"667ddecc-e660-4bd5-91b1-9fe0e3173e26","metadata":{"tags":[],"id":"667ddecc-e660-4bd5-91b1-9fe0e3173e26"},"outputs":[],"source":["y_pred, y_test, sentences = validation()\n","len(y_pred), len(y_test), len(sentences)"]},{"cell_type":"markdown","id":"02da65c3-d18a-4103-9e85-954234b2fbe6","metadata":{"id":"02da65c3-d18a-4103-9e85-954234b2fbe6"},"source":["# **Evaluate Model**\n","# **Transform Predictions:**\n","- ## **Output Model (Probability): (0.18, 0.40, 0.55, 0.88, 0.01)**\n","- ## **Output Transformed With Threshold == 0.5: (0, 0, 1, 1, 0)**\n","- ## **Output Transformed With Threshold == 0.6: (0, 0, 0, 1, 0)**\n","- ## **Output Transformed With Threshold == 0.3: (0, 1, 1, 1, 0)**"]},{"cell_type":"code","execution_count":null,"id":"a26822c5-6480-4678-860d-fd643f557e23","metadata":{"id":"a26822c5-6480-4678-860d-fd643f557e23"},"outputs":[],"source":["def transformPredictions(threshold, pred):\n","    r = []\n","    for e in pred:\n","        nt = ()\n","        for p in e:\n","            nt += (1 if p > threshold else 0,)\n","        r.append(nt)\n","    return r\n","       \n","y_pred_t = transformPredictions(0.5, y_pred)\n","y_test = list(map(tuple, y_test))"]},{"cell_type":"markdown","id":"9a600a06-7fdb-44dd-a2ee-2bfdb529a1c9","metadata":{"id":"9a600a06-7fdb-44dd-a2ee-2bfdb529a1c9"},"source":["# **Model Metrics**\n","## **Hamming Score and Hamming Loss: appropriate for multilabel problem**\n","## **falsePositives() function: considers all test data**"]},{"cell_type":"code","execution_count":null,"id":"520c6c78-8fc6-4ef1-910b-e6214dee8ee9","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"tags":[],"id":"520c6c78-8fc6-4ef1-910b-e6214dee8ee9"},"outputs":[],"source":["from sklearn.metrics import hamming_loss, accuracy_score, multilabel_confusion_matrix, f1_score, plot_confusion_matrix, precision_score, recall_score\n","import numpy as np\n","\n","def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n","    \n","    '''\n","    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n","    http://stackoverflow.com/q/32239577/395857\n","    '''\n","    \n","    acc_list = []\n","    for i in range(y_true.shape[0]):\n","        set_true = set( np.where(y_true[i])[0] )\n","        set_pred = set( np.where(y_pred[i])[0] )\n","        # print('\\nset_true: {0}'.format(set_true))\n","        # print('set_pred: {0}'.format(set_pred))\n","        tmp_a = None\n","        if len(set_true) == 0 and len(set_pred) == 0:\n","            tmp_a = 1\n","        else:\n","            tmp_a = len(set_true.intersection(set_pred))/\\\n","                    float( len(set_true.union(set_pred)) )\n","        #print('tmp_a: {0}'.format(tmp_a))\n","        acc_list.append(tmp_a)\n","    return np.mean(acc_list)\n","\n","# Calculate False Positives\n","def falsePositive(y_true, y_pred):\n","    total = 0\n","    fp = 0\n","    for i in range(len(y_pred)):\n","        for j in range(len(y_pred[i])):\n","            if (y_pred[i][j] == 1 and y_true[i][j] == 0):\n","                fp += 1\n","                break\n","        total += 1\n","    return fp/float(total)    \n","\n","\n","# Show Model Metrics\n","print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_t))\n","print(\"Precision Micro:\", precision_score(y_test, y_pred_t, average='micro'))\n","print(\"Precision Macro:\", precision_score(y_test, y_pred_t, average='macro'))\n","print(\"Recall Micro:\", recall_score(y_test, y_pred_t, average='micro'))\n","print(\"Recall Macro:\", recall_score(y_test, y_pred_t,average='macro'))\n","print(\"Hamming Score:\", hamming_score(np.array(y_test), y_pred_t))\n","print(\"Hamming Loss:\", hamming_loss(y_test, y_pred_t))\n","print(\"F1-Score Micro:\", f1_score(y_test, y_pred_t, average='micro'))\n","print(\"F1-Score Macro:\", f1_score(y_test, y_pred_t, average='macro'))\n","print(\"False Positives:\", falsePositive(y_test, y_pred_t))\n","\n","# Build Confusion Matrix\n","cms = multilabel_confusion_matrix(y_test, y_pred_t)"]},{"cell_type":"markdown","id":"426cf34a-952b-443c-87ac-640d6a03e9e1","metadata":{"id":"426cf34a-952b-443c-87ac-640d6a03e9e1"},"source":["# **Plot Confusion Matrix - SNS Heatmap**"]},{"cell_type":"code","execution_count":null,"id":"bce965be-79f3-4a1a-9160-90ffe24d863a","metadata":{"id":"bce965be-79f3-4a1a-9160-90ffe24d863a"},"outputs":[],"source":["import seaborn as sns\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=20):\n","\n","    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n","\n","    sns.set(font_scale=1.4)\n","\n","    try:\n","        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes, cmap='Blues')\n","    except ValueError:\n","        raise ValueError(\"Confusion matrix values must be integers.\")\n","        \n","    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n","    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n","    axes.set_ylabel('True label')\n","    axes.set_xlabel('Predicted label')\n","    axes.set_title(\"Confusion Matrix for the class - \" + class_label)\n","\n","def plotMultiLabelConfusionMatrices(cms, labels):\n","    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n","    for axes, conf_matrix, label in zip(ax.flatten(), cms, labels):\n","        print_confusion_matrix(conf_matrix, axes, label, [\"N\", \"Y\"])\n","        \n","labels = [] # Labels in alphabetic order\n","plotMultiLabelConfusionMatrices(cms, labels)"]},{"cell_type":"markdown","id":"36458748-0078-42f8-9831-35b858543ce1","metadata":{"id":"36458748-0078-42f8-9831-35b858543ce1"},"source":["# Saving Model"]},{"cell_type":"code","execution_count":null,"id":"ab094581-3558-4385-beac-cdb6caebe279","metadata":{"id":"ab094581-3558-4385-beac-cdb6caebe279"},"outputs":[],"source":["path = \"\" # Path Of Model\n","torch.save(model.state_dict(), path)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"bert_training.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}