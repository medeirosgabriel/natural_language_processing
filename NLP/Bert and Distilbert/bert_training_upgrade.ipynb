{"cells":[{"cell_type":"markdown","source":[""],"metadata":{"id":"P6KxPJI1_fDF"},"id":"P6KxPJI1_fDF"},{"cell_type":"markdown","metadata":{"id":"7d7af096-e29b-49d0-ad5d-8682159ea8c4"},"source":["# **DistilBERT Multilabel Training**\n","# **Link: https://www.kaggle.com/code/samson22/distilbert-in-pytorch**\n"],"id":"7d7af096-e29b-49d0-ad5d-8682159ea8c4"},{"cell_type":"markdown","metadata":{"id":"58160f17-2a9b-45e1-a06e-d3fee543fef1"},"source":["# **Data Format:**\n","# **Columns: Text, Labels**\n","## **Text: tuple of strings - Representing the context**\n","## **Tuple length 2 -> Context 2**\n","## **Tuple length 3 -> Context 3**\n","## **...**\n","## **Label: tuple of labels - Representing the multilabel classification**\n","\n","|       TEXT       |   LABEL   | \n","|------------------|-----------|\n","| (TEXT_1, TEXT_2) |  (LABEL_1, LABEL_2, ...)  |\n","| (TEXT_1, TEXT_2) |  (LABEL_1, LABEL_3, ...)  |\n","| (TEXT_1, TEXT_2) |  (LABEL_3)                |"],"id":"58160f17-2a9b-45e1-a06e-d3fee543fef1"},{"cell_type":"code","source":["from google.colab import files\n","files.upload()"],"metadata":{"id":"xbH6RZ-7L4wf"},"id":"xbH6RZ-7L4wf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","csv_path = \"\"\n","df = pd.read_csv(csv_path)\n","df.head(2)"],"metadata":{"id":"gSeAIKc64SiN"},"id":"gSeAIKc64SiN","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJHsQhtbVUfi"},"outputs":[],"source":["df.columns = ['text', 'label']\n","df.label = df.label.apply(lambda x: (x,))\n","df.head(2)"],"id":"ZJHsQhtbVUfi"},{"cell_type":"code","source":["df.label.value_counts()"],"metadata":{"id":"ImK2wuYpAK8q"},"id":"ImK2wuYpAK8q","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77e42867-b3ff-4545-9a78-3c3c577bee51"},"source":["# **Processing Labels**\n","# **Example: (label_1, label_3) -> (1, 0, 1)**\n","# **Transform labels to use in the Pytorch Model**"],"id":"77e42867-b3ff-4545-9a78-3c3c577bee51"},{"cell_type":"code","execution_count":null,"metadata":{"id":"6096a46e-f55f-4160-931c-a229c9b4834d"},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def labelsPreprocessing(labels):\n","    mlb = MultiLabelBinarizer()\n","    labels = mlb.fit_transform(labels)\n","    return labels, mlb\n","\n","labels, encoder = labelsPreprocessing(list(df.label.values))\n","df.label = list(map(lambda x: tuple(x), labels))\n","df.label.value_counts()"],"id":"6096a46e-f55f-4160-931c-a229c9b4834d"},{"cell_type":"code","source":["import random\n","\n","def reduceData(df, n=10000000):\n","    dic = df.groupby(by=\"label\").groups\n","    selected_texts = []\n","    selected_labels = []\n","    selected_id = []\n","    for k in dic.keys():\n","        if (len(dic[k]) > n):\n","            dic[k] = random.sample(list(dic[k]), n)\n","        for i in dic[k]:\n","            selected_labels.append(k)\n","            selected_texts.append(df.text[i])\n","    return pd.DataFrame(data={\"text\": selected_texts, \"label\": selected_labels})\n","\n","df = reduceData(df, 2000)\n","df.label.value_counts()"],"metadata":{"id":"ESFc99-dOGBP"},"id":"ESFc99-dOGBP","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"071ccff0-09db-478c-bae5-2a5651961c26"},"source":["# **Model Configuration - Pytorch**"],"id":"071ccff0-09db-478c-bae5-2a5651961c26"},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLsEYjGSVraX"},"outputs":[],"source":["!pip install torch\n","!pip install transformers"],"id":"sLsEYjGSVraX"},{"cell_type":"code","execution_count":null,"metadata":{"id":"59d1e77c-9894-4a97-bfbe-d07dbeb01a01"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, BertConfig"],"id":"59d1e77c-9894-4a97-bfbe-d07dbeb01a01"},{"cell_type":"markdown","metadata":{"id":"cd09f2c6-d00f-4b62-ae37-2a37a215c453"},"source":["# **Select the hardware to training the pytorch model**"],"id":"cd09f2c6-d00f-4b62-ae37-2a37a215c453"},{"cell_type":"code","execution_count":null,"metadata":{"id":"fd1a5053-1f9b-44c5-9172-be8bef68c60e"},"outputs":[],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu' # CPU OR GPU"],"id":"fd1a5053-1f9b-44c5-9172-be8bef68c60e"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"b0378629-17f6-4c21-ae05-bf5d9ac09314"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Distilbert Tokenizer"],"id":"b0378629-17f6-4c21-ae05-bf5d9ac09314"},{"cell_type":"markdown","metadata":{"id":"5afe70ae-4492-4d05-84e4-c5abc7b2d9e9"},"source":["# **Data Configuration to Pytorch Model Input**"],"id":"5afe70ae-4492-4d05-84e4-c5abc7b2d9e9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"287d2ebe-7611-4e9a-9ef0-4156b99066cf"},"outputs":[],"source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        # Class Attributes\n","        self.tokenizer = tokenizer # Tokenizer\n","        self.data = dataframe # All Data\n","        self.text = dataframe.text # Select text column\n","        self.label = self.data.label # Labels\n","        self.max_len = max_len # Max length of token list\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        \n","        # Build a context with sentences\n","        \n","        t = self.text[index] # Tuple (sentence_1, sentence_2, sentence_3)\n","        token_text = \"[SEP]\".join(t) + \"[SEP]\" # Join sentences\n","        final_text = \"[CLS]\" + token_text # Put [CLS] token at the beginning of the sentence \n","            \n","        inputs = tokenizer.encode_plus(\n","            final_text, # Text with special tokens ([CLS] and )\n","            None,\n","            add_special_tokens=False, # [CLS] and [SEP]\n","            max_length=self.max_len, # Select Max Length\n","            pad_to_max_length=True, # Complete Padding with ZEROS\n","            return_token_type_ids=None, # Token Type Ids -> Identify sentences separated by [SEP] token\n","            truncation=True\n","        )\n","        \n","        # Ids = TOKEN IDS\n","        ids = inputs['input_ids']\n","        # MASK = MASK of words if you are using\n","        mask = inputs['attention_mask']\n","        # Token Type Ids = Diff Sentences\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        \n","        # Format the output for the model training\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'label': torch.tensor(self.label[index], dtype=torch.float),\n","            'sentence': final_text\n","        }"],"id":"287d2ebe-7611-4e9a-9ef0-4156b99066cf"},{"cell_type":"markdown","metadata":{"id":"c094fccd-178d-40b2-8a7e-580ba54f2359"},"source":["# **Data Configuration To Pytorch**"],"id":"c094fccd-178d-40b2-8a7e-580ba54f2359"},{"cell_type":"code","execution_count":null,"metadata":{"id":"56f9d34c-e5b5-43d2-8bb7-0b678260676b"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Split dataset according to LABEL column\n","train_dataset, test_dataset = train_test_split(df, test_size = 0.2, stratify=df['label'])\n","train_dataset = train_dataset.reset_index().drop(columns=[\"index\"], axis=1)\n","test_dataset = test_dataset.reset_index().drop(columns=[\"index\"], axis=1)"],"id":"56f9d34c-e5b5-43d2-8bb7-0b678260676b"},{"cell_type":"markdown","metadata":{"id":"10f5e16e-2e2f-430b-88f2-4b0f60545393"},"source":["# **Dataset Shapes - full, train and test**"],"id":"10f5e16e-2e2f-430b-88f2-4b0f60545393"},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ad02650-9160-4b75-b475-1eb51b31381c"},"outputs":[],"source":["print(\"FULL Dataset: {}\".format(df.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(test_dataset.shape))\n","\n","MAX_LEN = 512 # Tokens Length\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Distilbert Tokenizer\n","\n","training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n","testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)"],"id":"5ad02650-9160-4b75-b475-1eb51b31381c"},{"cell_type":"markdown","metadata":{"id":"b77cd766-bfee-4e45-8fb6-fe83d00503a2"},"source":["# **Train and Test Parameters**"],"id":"b77cd766-bfee-4e45-8fb6-fe83d00503a2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e6d57fc7-38db-48a1-a4ba-3fbbf0155624"},"outputs":[],"source":["TRAIN_BATCH_SIZE = 8 # Train Batch Size\n","VALID_BATCH_SIZE = 4 # Valid Batch Size\n","\n","train_params = {\n","    'batch_size': TRAIN_BATCH_SIZE,           \n","    'shuffle': True,           \n","    'num_workers': 0\n","}\n","\n","test_params = {\n","    'batch_size': VALID_BATCH_SIZE,\n","    'shuffle': True,\n","    'num_workers': 0\n","}\n","\n","training_loader = DataLoader(training_set, **train_params)\n","testing_loader = DataLoader(testing_set, **test_params)"],"id":"e6d57fc7-38db-48a1-a4ba-3fbbf0155624"},{"cell_type":"markdown","metadata":{"id":"657e88b0-5364-4287-b52a-61ef62b85162"},"source":["# **Build DistilBERT Model**"],"id":"657e88b0-5364-4287-b52a-61ef62b85162"},{"cell_type":"code","execution_count":null,"metadata":{"id":"93895420-b598-419d-96e2-d0526c3aa559"},"outputs":[],"source":["class BERTClass(torch.nn.Module):\n","    def __init__(self, config):\n","        super(BERTClass, self).__init__()\n","        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n","        self.l2 = torch.nn.Dropout(config.dropout)\n","        self.l3 = torch.nn.Linear(config.hidden_size, config.num_labels)\n","    \n","    def forward(self, ids, mask, token_type_ids):\n","        _, output_1 = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n","        output_2 = self.l2(output_1)\n","        output = self.l3(output_2)\n","        return output\n","\n","NUM_LABELS = len(test_dataset.label.values[0])\n","\n","config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768, dropout=0.1, num_labels=NUM_LABELS,\n","        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n","\n","model = BERTClass(config)\n","model.to(device) # Choose the device (CPU or GPU)"],"id":"93895420-b598-419d-96e2-d0526c3aa559"},{"cell_type":"markdown","metadata":{"id":"6770c221-5f5e-4e02-9d20-834126845272"},"source":["# **Training DistilBERT Model**"],"id":"6770c221-5f5e-4e02-9d20-834126845272"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZjqteekeIis"},"outputs":[],"source":["import copy"],"id":"yZjqteekeIis"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0743202c-dfc4-4e4e-875a-59b729b4dde6"},"outputs":[],"source":["def train_model(model, criterion, optimizer, scheduler, num_labels, num_epochs=2):\n","    model.train()\n","    since = time.time()\n","    print('starting')\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = 100\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                scheduler.step()\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            \n","            beta_score_accuracy = 0.0\n","            \n","            micro_roc_auc_acc = 0.0\n","            \n","            # Iterate over data.\n","            for _,data in enumerate(dataloaders_dict[phase], 0):\n","                ids = data['ids'].to(device, dtype = torch.long)\n","                mask = data['mask'].to(device, dtype = torch.long)\n","                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","                labels = data['label'].to(device, dtype = torch.float)\n","                \n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(ids, mask, token_type_ids)\n","                    \n","                    loss = criterion(outputs, labels)\n","                    \n","                    if phase == 'train':\n","                        \n","                        loss.backward()\n","                        optimizer.step()\n","\n","                running_loss += loss.item() * ids.size(0)\n","                \n","                micro_roc_auc_acc +=  accuracy_thresh(outputs.view(-1,num_labels), labels.view(-1,num_labels))\n","\n","                \n","            epoch_loss = running_loss / dataset_sizes[phase]\n","\n","            epoch_micro_roc_acc = micro_roc_auc_acc / dataset_sizes[phase]\n","\n","            print('{} total loss: {:.4f} '.format(phase,epoch_loss ))\n","            print('{} micro_roc_auc_acc: {:.4f}'.format( phase, epoch_micro_roc_acc))\n","\n","            if phase == 'val' and epoch_loss < best_loss:\n","                print('saving with loss of {}'.format(epoch_loss), 'improved over previous {}'.format(best_loss))\n","                best_loss = epoch_loss\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                torch.save(model.state_dict(), 'distilbert_model_weights.pth')\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(float(best_loss)))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"],"id":"0743202c-dfc4-4e4e-875a-59b729b4dde6"},{"cell_type":"code","source":["dataloaders_dict = {\n","    'train': training_loader,\n","    'val':testing_loader\n","}\n","\n","dataset_sizes = {\n","    'train':train_dataset.shape[0],\n","    'val':test_dataset.shape[0]\n","}"],"metadata":{"id":"t8CkKIFuZ-7l"},"id":"t8CkKIFuZ-7l","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def accuracy_thresh(y_pred, y_true, thresh:float=0.4, sigmoid:bool=True):\n","    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n","    if sigmoid: y_pred = y_pred.sigmoid()\n","    return np.mean(((y_pred>thresh).float()==y_true.float()).float().cpu().numpy(), axis=1).sum()\n","\n","def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"],"metadata":{"id":"FkesnVB8_Lcz"},"id":"FkesnVB8_Lcz","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89d0f553-42eb-4838-a739-474766b9024f"},"outputs":[],"source":["from torch.optim import lr_scheduler\n","import time\n","\n","criterion = torch.nn.BCEWithLogitsLoss() # Loss Function\n","EPOCHS = 10 # Epochs\n","LEARNING_RATE = 1e-05 # Learning Rate\n","\n","optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","model = train_model(model, criterion, optimizer, exp_lr_scheduler, NUM_LABELS, num_epochs=EPOCHS)"],"id":"89d0f553-42eb-4838-a739-474766b9024f"},{"cell_type":"markdown","metadata":{"id":"26f5d4fc-ff5e-4012-a01e-ee3c0aaab6d2"},"source":["# **Predict Test Data - Model**"],"id":"26f5d4fc-ff5e-4012-a01e-ee3c0aaab6d2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2835943-d20c-4681-9656-c154f7ee8b8d"},"outputs":[],"source":["def validation():\n","    model.eval()\n","    fin_targets = []\n","    fin_outputs = []\n","    sentences = []\n","    annotation_ids = []\n","    with torch.no_grad():\n","        for _, data in enumerate(testing_loader, 0):\n","            ids = data['ids'].to(device, dtype = torch.long) # Tokens\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long) # Masks\n","            targets = data['label'].to(device, dtype = torch.float) # True Label\n","            outputs = model(ids, mask, token_type_ids) # Predict\n","            # Prepare Output in lists\n","            sentences.extend(data['sentence']) # predicted texts\n","            fin_targets.extend(targets.cpu().detach().numpy().tolist())  # true labels of predicted texts\n","            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())  # predicted labels of predicted texts\n","    return fin_outputs, fin_targets, sentences # Output, True Label, Sentences, IDS"],"id":"a2835943-d20c-4681-9656-c154f7ee8b8d"},{"cell_type":"code","execution_count":null,"metadata":{"id":"667ddecc-e660-4bd5-91b1-9fe0e3173e26"},"outputs":[],"source":["y_pred, y_test, sentences = validation()\n","len(y_pred), len(y_test), len(sentences)"],"id":"667ddecc-e660-4bd5-91b1-9fe0e3173e26"},{"cell_type":"markdown","metadata":{"id":"02da65c3-d18a-4103-9e85-954234b2fbe6"},"source":["# **Evaluate Model**\n","# **Transform Predictions:**\n","- ## **Output Model (Probability): (0.18, 0.40, 0.55, 0.88, 0.01)**\n","- ## **Output Transformed With Threshold == 0.5: (0, 0, 1, 1, 0)**\n","- ## **Output Transformed With Threshold == 0.6: (0, 0, 0, 1, 0)**\n","- ## **Output Transformed With Threshold == 0.3: (0, 1, 1, 1, 0)**"],"id":"02da65c3-d18a-4103-9e85-954234b2fbe6"},{"cell_type":"code","execution_count":null,"metadata":{"id":"a26822c5-6480-4678-860d-fd643f557e23"},"outputs":[],"source":["def transformPredictions(threshold, pred):\n","    r = []\n","    for e in pred:\n","        nt = ()\n","        for p in e:\n","            nt += (1 if p > threshold else 0,)\n","        r.append(nt)\n","    return r\n","       \n","y_pred_t = transformPredictions(0.5, y_pred)\n","y_test = list(map(tuple, y_test))"],"id":"a26822c5-6480-4678-860d-fd643f557e23"},{"cell_type":"markdown","metadata":{"id":"9a600a06-7fdb-44dd-a2ee-2bfdb529a1c9"},"source":["# **Model Metrics**\n","## **Hamming Score and Hamming Loss: appropriate for multilabel problem**\n","## **falsePositives() function: considers all test data**"],"id":"9a600a06-7fdb-44dd-a2ee-2bfdb529a1c9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"520c6c78-8fc6-4ef1-910b-e6214dee8ee9"},"outputs":[],"source":["from sklearn.metrics import hamming_loss, accuracy_score, multilabel_confusion_matrix, f1_score, plot_confusion_matrix, precision_score, recall_score\n","import numpy as np\n","\n","def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n","    \n","    '''\n","    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n","    http://stackoverflow.com/q/32239577/395857\n","    '''\n","    \n","    acc_list = []\n","    for i in range(y_true.shape[0]):\n","        set_true = set( np.where(y_true[i])[0] )\n","        set_pred = set( np.where(y_pred[i])[0] )\n","        # print('\\nset_true: {0}'.format(set_true))\n","        # print('set_pred: {0}'.format(set_pred))\n","        tmp_a = None\n","        if len(set_true) == 0 and len(set_pred) == 0:\n","            tmp_a = 1\n","        else:\n","            tmp_a = len(set_true.intersection(set_pred))/\\\n","                    float( len(set_true.union(set_pred)) )\n","        #print('tmp_a: {0}'.format(tmp_a))\n","        acc_list.append(tmp_a)\n","    return np.mean(acc_list)\n","\n","# Calculate False Positives\n","def falsePositive(y_true, y_pred):\n","    total = 0\n","    fp = 0\n","    for i in range(len(y_pred)):\n","        for j in range(len(y_pred[i])):\n","            if (y_pred[i][j] == 1 and y_true[i][j] == 0):\n","                fp += 1\n","                break\n","        total += 1\n","    return fp/float(total)    \n","\n","\n","# Show Model Metrics\n","print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_t))\n","print(\"Precision Micro:\", precision_score(y_test, y_pred_t, average='micro'))\n","print(\"Precision Macro:\", precision_score(y_test, y_pred_t, average='macro'))\n","print(\"Recall Micro:\", recall_score(y_test, y_pred_t, average='micro'))\n","print(\"Recall Macro:\", recall_score(y_test, y_pred_t,average='macro'))\n","print(\"Hamming Score:\", hamming_score(np.array(y_test), y_pred_t))\n","print(\"Hamming Loss:\", hamming_loss(y_test, y_pred_t))\n","print(\"F1-Score Micro:\", f1_score(y_test, y_pred_t, average='micro'))\n","print(\"F1-Score Macro:\", f1_score(y_test, y_pred_t, average='macro'))\n","print(\"False Positives:\", falsePositive(y_test, y_pred_t))\n","\n","# Build Confusion Matrix\n","cms = multilabel_confusion_matrix(y_test, y_pred_t)"],"id":"520c6c78-8fc6-4ef1-910b-e6214dee8ee9"},{"cell_type":"markdown","metadata":{"id":"426cf34a-952b-443c-87ac-640d6a03e9e1"},"source":["# **Plot Confusion Matrix - SNS Heatmap**"],"id":"426cf34a-952b-443c-87ac-640d6a03e9e1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bce965be-79f3-4a1a-9160-90ffe24d863a"},"outputs":[],"source":["import seaborn as sns\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=20):\n","\n","    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n","\n","    sns.set(font_scale=1.4)\n","\n","    try:\n","        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes, cmap='Blues')\n","    except ValueError:\n","        raise ValueError(\"Confusion matrix values must be integers.\")\n","        \n","    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n","    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n","    axes.set_ylabel('True label')\n","    axes.set_xlabel('Predicted label')\n","    axes.set_title(\"Confusion Matrix for the class - \" + class_label)\n","\n","def plotMultiLabelConfusionMatrices(cms, labels):\n","    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n","    for axes, conf_matrix, label in zip(ax.flatten(), cms, labels):\n","        print_confusion_matrix(conf_matrix, axes, label, [\"N\", \"Y\"])\n","        \n","labels = ['ALEGRIA', 'MEDO'] # Labels in alphabetic order\n","plotMultiLabelConfusionMatrices(cms, labels)"],"id":"bce965be-79f3-4a1a-9160-90ffe24d863a"},{"cell_type":"markdown","metadata":{"id":"36458748-0078-42f8-9831-35b858543ce1"},"source":["# Saving Model"],"id":"36458748-0078-42f8-9831-35b858543ce1"},{"cell_type":"code","execution_count":null,"metadata":{"id":"ab094581-3558-4385-beac-cdb6caebe279"},"outputs":[],"source":["path = \"\" # Path Of Model\n","torch.save(model.state_dict(), path)"],"id":"ab094581-3558-4385-beac-cdb6caebe279"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"bert_training_update.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}