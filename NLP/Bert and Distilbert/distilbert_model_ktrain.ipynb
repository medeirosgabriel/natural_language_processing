{"cells":[{"cell_type":"markdown","metadata":{"id":"gJgEDmuj-IUt"},"source":["# Distilbert Model"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"wXNSiXMw-IU2"},"outputs":[],"source":["import tensorflow as tf\n","import os\n","\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n","tf.config.list_physical_devices('GPU')\n","tf.test.is_built_with_cuda()"]},{"cell_type":"markdown","metadata":{"id":"rKQAQkbntUC0"},"source":["# Load Data"]},{"cell_type":"code","source":["import pandas as pd\n","\n","csv_path = \"\"\n","data = pd.read_csv(csv_path)\n","data.head(5)"],"metadata":{"id":"mFgZ7y-MEEa-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mCWvfrZVouzO"},"outputs":[],"source":["data.columns = ['text', 'label']\n","data.head(5)"]},{"cell_type":"markdown","source":["# Translate Sentence To English - Optional"],"metadata":{"id":"DU6iUZ2MBUiv"}},{"cell_type":"code","source":["#!pip install deep_translator"],"metadata":{"id":"qPU5yca3AKXc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from deep_translator import GoogleTranslator\n","translator = GoogleTranslator(source='auto', target='en')\n","\n","def translateSplittedTopics(sentence, translator):\n","  return translator.translate(sentence)\n","\n","data.text = data.text.apply(lambda x: translateSplittedTopics(x, translator))"],"metadata":{"id":"9S0aX9W0_5Av"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.label = data.label.apply(lambda x: (x,) if type(x) != tuple else x)\n","data.head(5)"],"metadata":{"id":"mtA1Wz1s-zd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZdOa_lVd-IVG"},"outputs":[],"source":["df = data.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9jQzeVc-IVK"},"outputs":[],"source":["import numpy as np\n","df.label.value_counts()"]},{"cell_type":"markdown","metadata":{"id":"lKz3TJC8-IVN"},"source":["# Label Analysis"]},{"cell_type":"code","source":["#!pip install pyyaml==5.4.1"],"metadata":{"id":"hQHdoeICFPQ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"V6uwMz23-IVQ"},"outputs":[],"source":["import plotly.express as px\n","df_test = df.label.apply(lambda x: str(x))\n","fig = px.histogram(df_test, x=\"label\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"wZEi50nh-IVW"},"source":["# Reduce Data"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"sSYEbbQC-IVW"},"outputs":[],"source":["import random\n","\n","def reduce_data(df, n=10000000):\n","    dic = df.groupby(by=\"label\").groups\n","    selected_texts = []\n","    selected_labels = []\n","    for k in dic.keys():\n","        if (len(dic[k]) > n):\n","            dic[k] = random.sample(list(dic[k]), n)\n","        for i in dic[k]:\n","            selected_labels.append(k)\n","            selected_texts.append(df.text[i])\n","    return pd.DataFrame(data={\"text\": selected_texts, \"label\": selected_labels})\n","\n","df = reduce_data(df, 2000)\n","df.label.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-skAAyR9-IVX"},"outputs":[],"source":["df_test = df.label.apply(lambda x: str(x))\n","fig = px.histogram(df_test, x=\"label\")\n","fig.show()"]},{"cell_type":"markdown","metadata":{"id":"oh1Q607X-IVY"},"source":["# Text Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g7EWdTzX-IVZ"},"outputs":[],"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import string\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","def removePunctuation(t):\n","    punc = string.punctuation\n","    for e in t:\n","        if e in punc:\n","            t = t.replace(e, \"\")\n","    return t\n","\n","def removeStopWordsStemmer(sentence, ps):\n","    sentence_tokens = word_tokenize(sentence)\n","    sentence_without_sw = [word for word in sentence_tokens if not word in stopwords.words()]\n","    sentence_without_sw = list(map(lambda s: ps.stem(s), sentence_without_sw))\n","    return \" \".join(sentence_without_sw)\n","\n","ps = PorterStemmer()\n","df.text = df.text.apply(lambda t: t.lower())\n","df.text = df.text.apply(lambda t: removePunctuation(t))\n","df.text = df.text.apply(lambda t: removeStopWordsStemmer(t, ps))"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"nUUuUYiK-IVb"},"outputs":[],"source":["class_names = []\n","\n","for label in list(df.label.unique()):\n","  l = label[0]\n","  class_names += [l]\n","\n","class_names"]},{"cell_type":"markdown","metadata":{"id":"714bkhha-IVc"},"source":["# Labels Preprocessing"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"Cg33aJhA-IVd","executionInfo":{"status":"ok","timestamp":1649722580239,"user_tz":180,"elapsed":359,"user":{"displayName":"Gabriel Medeiros","userId":"00840483478177331557"}}},"outputs":[],"source":["from sklearn.preprocessing import MultiLabelBinarizer\n","\n","def labelsPreprocessing(labels):\n","    mlb = MultiLabelBinarizer()\n","    labels = mlb.fit_transform(df['label'])\n","    return labels, mlb\n","\n","labels, encoder = labelsPreprocessing(list(df.label.values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BmyAoZZY-IVd"},"outputs":[],"source":["df.label = list(labels)\n","df.label = df.label.apply(lambda x: tuple(x))\n","df.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"CzaMnOhW-IVe"},"source":["# Training Model"]},{"cell_type":"code","source":["!pip install ktrain"],"metadata":{"id":"wL3wVa9KCfB_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"XcKOxMhc-IVe"},"outputs":[],"source":["import ktrain\n","from ktrain import text\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(df['text'], df['label'], shuffle=True, test_size = 0.2)\n","x_train, y_train = list(x_train), list(y_train)\n","x_test, y_test = list(x_test), list(y_test)\n","\n","trn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,\n","                                          x_test=x_test, y_test=y_test,\n","                                          class_names = class_names,\n","                                          preprocess_mode='distilbert',\n","                                          maxlen=200)\n","\n","# distilbert, distilbert-base-cased, distilbert-base-multilingual-cased\n","MODEL_NAME = 'distilbert'\n","model = text.text_classifier(MODEL_NAME, train_data=trn, preproc=preproc, multilabel=True) \n","\n","#model.distilbert.trainable = False\n","#model.pre_classifier.trainable = False\n","#model.classifier.trainable = False\n","#model = t.get_classifier(metrics=['accuracy'])\n","model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n","learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=3)\n","\n","model.summary()\n","learner.fit_onecycle(0.00001, 10)\n","predictor = ktrain.get_predictor(learner.model, preproc=preproc)\n","y_pred = predictor.predict(x_test)\n","y_pred[0:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"d7LHOHq--IVg"},"outputs":[],"source":["def transformPredictions(pred, class_names):\n","  r = []\n","  for p in pred:\n","    nt = ()\n","    for c in class_names:\n","      nt += (1 if c in p else 0,)\n","    r.append(nt)\n","  return r\n","        \n","y_pred_t = transformPredictions(y_pred, class_names)\n","y_pred_t[0:2], y_test[0:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"WSNnIGSf-IVi"},"outputs":[],"source":["from sklearn.metrics import hamming_loss, accuracy_score, multilabel_confusion_matrix, f1_score, plot_confusion_matrix, precision_score, recall_score\n","import numpy as np\n","\n","def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n","    \n","    '''\n","    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n","    http://stackoverflow.com/q/32239577/395857\n","    '''\n","    \n","    acc_list = []\n","    for i in range(y_true.shape[0]):\n","        set_true = set( np.where(y_true[i])[0] )\n","        set_pred = set( np.where(y_pred[i])[0] )\n","        #print('\\nset_true: {0}'.format(set_true))\n","        #print('set_pred: {0}'.format(set_pred))\n","        tmp_a = None\n","        if len(set_true) == 0 and len(set_pred) == 0:\n","            tmp_a = 1\n","        else:\n","            tmp_a = len(set_true.intersection(set_pred))/\\\n","                    float( len(set_true.union(set_pred)) )\n","        #print('tmp_a: {0}'.format(tmp_a))\n","        acc_list.append(tmp_a)\n","    return np.mean(acc_list)\n","\n","def falsePositive(y_true, y_pred):\n","    total = 0\n","    fp = 0\n","    for i in range(len(y_pred)):\n","        for j in range(len(y_pred[i])):\n","            if (y_pred[i][j] == 1 and y_true[i][j] == 0):\n","                fp += 1\n","                break\n","        total += 1\n","    return fp/float(total)       \n","\n","print(\"Accuracy Score:\", accuracy_score(y_test, y_pred_t))\n","print(\"Precision Micro:\", precision_score(y_test, y_pred_t, average='micro'))\n","print(\"Precision Macro:\", precision_score(y_test, y_pred_t, average='macro'))\n","print(\"Recall Micro:\", recall_score(y_test, y_pred_t, average='micro'))\n","print(\"Recall Macro:\", recall_score(y_test, y_pred_t,average='macro'))\n","print(\"Hamming Score:\", hamming_score(np.array(y_test), y_pred_t))\n","print(\"Hamming Loss:\", hamming_loss(y_test, y_pred_t))\n","print(\"F1-Score Micro:\", f1_score(y_test, y_pred_t, average='micro'))\n","print(\"F1-Score Macro:\", f1_score(y_test, y_pred_t, average='macro'))\n","print(\"False Positives:\", falsePositive(y_test, y_pred_t))\n","cms = multilabel_confusion_matrix(y_test, y_pred_t)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"5-1hy9Bj-IVj"},"outputs":[],"source":["import seaborn as sns\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=20):\n","\n","    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n","\n","    sns.set(font_scale=1.4)\n","\n","    try:\n","        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes, cmap='Blues')\n","    except ValueError:\n","        raise ValueError(\"Confusion matrix values must be integers.\")\n","        \n","    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n","    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n","    axes.set_ylabel('True label')\n","    axes.set_xlabel('Predicted label')\n","    axes.set_title(\"Confusion Matrix for the class - \" + class_label)\n","\n","def plotMultiLabelConfusionMatrices(cms, labels):\n","    fig, ax = plt.subplots(1, 2, figsize=(22, 10))\n","    for axes, conf_matrix, label in zip(ax.flatten(), cms, labels):\n","        print_confusion_matrix(conf_matrix, axes, label, [\"N\", \"Y\"])\n","        \n","labels = ['ALEGRIA', 'MEDO'] # Labels in alphabetic order\n","plotMultiLabelConfusionMatrices(cms, labels)"]},{"cell_type":"markdown","metadata":{"id":"7xOpuBfQ-IVk"},"source":["# Save Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZony_1W-IVl"},"outputs":[],"source":["#Save Model\n","\n","#predictor.save(\"./distilbert\")\n","\n","#Load Model\n","\n","'''\n","import ktrain\n","from ktrain import text\n","path = \"./distilbert\"\n","loaded_predictor = ktrain.load_predictor(path)\n","loaded_predictor.predict(['we can close the chat'])\n","'''"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"distilbert_model_ktrain.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}